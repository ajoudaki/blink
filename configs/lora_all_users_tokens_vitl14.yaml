# Configuration for LoRA fine-tuning with labeler-specific tokens - ViT-L/14

# CLIP model configuration - Using larger ViT-L/14 (~428M params)
clip_model: "ViT-L/14"

# LoRA configuration
lora:
  rank: 8
  alpha: 4.0
  temperature: 0.15
  # Visual prompts: Single token per user (like CLS)

# Data configuration
data:
  num_workers: 4
  augment_text: true
  val_split: 0.3
  test_split: 0.3

# User filtering configuration
min_samples_per_user: 100  # Minimum samples required per user
max_users: null  # Set to None for all users, or a number to limit

# Training configuration
training:
  batch_size: 16  # Much smaller for ViT-L/14
  eval_batch_size: 32  # Much smaller for ViT-L/14
  learning_rate: 1e-4
  weight_decay: 0.01
  max_epochs: 20  # More epochs for convergence with more users
  eval_every: 1

# GPU configuration - Use GPU 1
gpu:
  device_id: 1

# Text templates for augmentation
text_templates:
  - "{}"  # Just the bare word
  - "a {} person"
  - "this person looks {}"
  - "this person is {}"
  - "this person appears {}"

# Seed for reproducibility
seed: 42