# LoRA single-user configuration for fast testing
# Uses smallest CLIP model and single user's data

# CLIP model to use - ViT-B/32 is the basic Vision Transformer
clip_model: "ViT-B/32"  # Basic ViT model

# LoRA specific parameters
lora:
  rank: 4  # LoRA rank
  alpha: 1.0  # LoRA scaling factor
  temperature: 0.1  # Higher temperature for better generalization (was 0.07)

# Training parameters
training:
  max_epochs: 20  # Extended training
  batch_size: 384  # Larger batch size (50% increase from 256)
  learning_rate: 0.0005  # Higher LR for faster convergence
  weight_decay: 0.01  # Weight decay
  patience: 5  # Early stopping patience
  min_delta: 0.001  # Minimum improvement
  eval_batch_size: 768  # Larger eval batch
  gradient_checkpointing: false  # Disabled due to CLIP compatibility

# Data parameters
data:
  val_split: 0.15  # Validation split
  test_split: 0.15  # Test split
  num_workers: 2  # Data loading workers
  single_user: "5fc68c7d781dffc92b8a11e5"  # Top user with 4602 labels

# GPU settings
gpu:
  device_id: 1

# Random seed
seed: 42