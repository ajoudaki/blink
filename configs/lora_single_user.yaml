# LoRA single-user configuration for fast testing
# Uses smallest CLIP model and single user's data

# CLIP model to use - ViT-B/32 is the basic Vision Transformer
clip_model: "ViT-B/32"  # Basic ViT model

# LoRA specific parameters
lora:
  rank: 4  # LoRA rank
  alpha: 1.0  # LoRA scaling factor
  temperature: 0.15  # Increased for better generalization (was 0.1, originally 0.07)
  enable_vision: true  # Enable vision fine-tuning (LoRA + visual prompts)
  enable_text: false  # Enable text fine-tuning (LoRA + text user tokens)

# Training parameters
training:
  max_epochs: 20  # Extended training
  batch_size: 384  # Larger batch size (50% increase from 256)
  learning_rate: 0.0005  # Higher LR for faster convergence
  weight_decay: 0.01  # Weight decay
  patience: 5  # Early stopping patience
  min_delta: 0.001  # Minimum improvement
  eval_batch_size: 768  # Larger eval batch
  gradient_checkpointing: false  # Disabled due to CLIP compatibility

# Data parameters
data:
  val_split: 0.15  # Validation split
  test_split: 0.15  # Test split
  num_workers: 2  # Data loading workers
  single_user: "5fc68c7d781dffc92b8a11e5"  # Top user with 4602 labels

# GPU settings
gpu:
  device_id: 1

# Random seed
seed: 42

# Text augmentation templates (randomly selected per batch)
text_templates:
  - "{}"  # Just the bare word (attractive, smart, trustworthy)
  - "a {} person"
  - "this person looks {}"
  - "this person is {}"
  - "this person appears {}"

# Image augmentation settings
enable_image_augmentation: false  # Disabled - only using text augmentation

# Performance optimizations
cache_images: false  # Cache preprocessed images in memory (set true if RAM allows)
use_mixed_precision: false  # Use fp16 mixed precision training for speed